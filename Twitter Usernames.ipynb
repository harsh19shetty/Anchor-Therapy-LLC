{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We obtain the consumer and access keys for Twitter API\n",
    "consumer_key=\"TVuAh0OA9xV3kfEVKKmgIwZjR\"\n",
    "consumer_secret=\"bRByFyYGkVmAqv8pKi6zpkvbdRxsX2PkfSLPx4pT5BeJipjYHD\"\n",
    "access_token=\"846893262-HbNLJbca0QatnG3IdiQVLIOoDN3Vxa983yKLo1XW\"\n",
    "access_token_secret=\"BVWlfKYB4JHlFSSeRN8cmJTyzzNWKLG841E9Vqwjgkf66\"\n",
    "\n",
    "auth=tw.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "\n",
    "## Connecting to the API using the Tweepy library\n",
    "api=tw.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can search for specific words in the tweet contents\n",
    "## Like \"Depression\", we can search for \"Anxiety\",\"Anxious\",\"Mental\", etc.\n",
    "search_words_1=\"Depression\"\n",
    "\n",
    "\n",
    "## Filter the retweets, as they would just be repititions\n",
    "new_search_1=search_words_1+\" -filter:retweets\"\n",
    "date_since = \"2018-7-1\"\n",
    "\n",
    "\n",
    "## Using the cursor object, we also filter the tweets in the Hoboken and Jersey City area\n",
    "## We require the Longitude, Latitude and the radius of the area\n",
    "tweets_1 = tw.Cursor(api.search,q=new_search_1,lang=\"en\",since=date_since,location=(40.7178,-74.0431,21.13)).items(200)\n",
    "\n",
    "\n",
    "## Using the for loop, we get the tweet and the user name\n",
    "user_locs_1 = [[tweet.user.screen_name, tweet.text] for tweet in tweets_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The tweet and user name are then converted to a dataframe\n",
    "tweet_text_1=pd.DataFrame(data=user_locs_1,columns=[\"user\",\"Tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The dataframe is saved as a CSV\n",
    "tweet_text_1.to_csv(r\"C:\\Users\\harsh\\Google Drive\\Stevens\\Sem 3\\686 - Practicum in Analytics\\Anchor\\D1.csv\", index = False)\n",
    "\n",
    "\n",
    "\n",
    "## The csv contain the tweet and the usernames\n",
    "## The same process is repeated for different words and locations\n",
    "## Finally, the usernames are collected in a seperate csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
